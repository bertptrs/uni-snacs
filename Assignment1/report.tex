\documentclass[12pt,a4paper,hidelinks]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{parskip}
\usepackage{hyperref}

\usepackage{listings}
\lstset{
	breaklines=true,
	tabsize=4,
	numbers=left
}
\author{Bert Peters --- s1147919}
\title{Social Network Analysis for Computer Scientists --- Assignment 1}
\begin{document}

\maketitle

\section{Exercise 1: Neighbourhoods}

\section{Exercise 3: An Online Social Network}

\subsection{Question 1: Number of edges}

As the file contains one link per line, the number of links can be determined by counting the number of lines. This does not require a program, but can be done with a little snippet of bash.

\lstinputlisting[language=bash]{edges.sh}

Or, rather, just use the \texttt{wc} utility. The results for this exercise are incorporated in \autoref{tab:counts}.

\subsection{Question 2: Number of nodes}

Like for the previous exercise, a full fledged parser is still not necessary. Instead, I combine the two columns using \texttt{awk}, \texttt{sort} them, take unique rows, and finally count lines again. The exact code is shown below, and the results can be found in \autoref{tab:counts}.

\lstinputlisting[language=bash]{nodes.sh}

The parameters for sort are slightly unusual. We do this to improve performance, so that this script can handle the \texttt{huge.in} network. The compress program allows us to use less disk space for temporaries, which is neccessary to prevent disk space issues, and we increase the buffer size to improve overall performance.

\begin{table}
\centering
\begin{tabular}{l | r | r}
Filename & {\centering $|E|$} & $|V|$ \\
\hline
medium.in & 16631 & 2426 \\
large.in & 14855842 & 456626 \\
huge.in & 892263106 & 8113017
\end{tabular}

\caption{Various counts for the network files}
\label{tab:counts}
\end{table}
\end{document}
